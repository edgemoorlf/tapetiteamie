import os
import json
import struct
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from werkzeug.utils import secure_filename
import dashscope
from dashscope.audio.asr import Recognition, RecognitionCallback
from dashscope import Generation
from dotenv import load_dotenv
import tempfile
import logging
from threading import Lock

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder='public', static_url_path='')
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# é…ç½®
DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')
dashscope.api_key = DASHSCOPE_API_KEY

UPLOAD_FOLDER = 'videos'
TEMP_AUDIO_FOLDER = 'temp_audio'
ALLOWED_VIDEO_EXTENSIONS = {'mp4', 'avi', 'mov'}
ALLOWED_AUDIO_EXTENSIONS = {'webm', 'wav', 'mp3', 'pcm'}

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(TEMP_AUDIO_FOLDER, exist_ok=True)
os.makedirs('public', exist_ok=True)

def allowed_file(filename, allowed_extensions):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in allowed_extensions

@app.route('/')
def index():
    return send_from_directory('public', 'index.html')

@app.route('/api/videos', methods=['GET'])
def get_videos():
    """è·å–æ‰€æœ‰è§†é¢‘åˆ—è¡¨åŠå…¶å­—å¹•"""
    try:
        if not os.path.exists(UPLOAD_FOLDER):
            return jsonify([])

        videos = []
        for filename in os.listdir(UPLOAD_FOLDER):
            if filename.endswith('.mp4'):
                video_info = {
                    'name': filename,
                    'url': f'/videos/{filename}'
                }

                # Check for transcript file (.txt with same name)
                transcript_path = os.path.join(UPLOAD_FOLDER, filename.replace('.mp4', '.txt'))
                if os.path.exists(transcript_path):
                    try:
                        with open(transcript_path, 'r', encoding='utf-8') as f:
                            video_info['transcript'] = f.read().strip()
                            logger.info(f"Loaded transcript for {filename}: {len(video_info['transcript'])} chars")
                    except Exception as e:
                        logger.warning(f"Failed to load transcript for {filename}: {e}")

                videos.append(video_info)

        # Sort videos: introduction.mp4 first, then alphabetically
        def sort_key(video):
            name = video['name'].lower()
            if name == 'introduction.mp4':
                return (0, '')  # First priority
            else:
                return (1, name)  # Alphabetical order

        videos.sort(key=sort_key)

        logger.info(f"Loaded {len(videos)} videos in order: {[v['name'] for v in videos]}")

        return jsonify(videos)
    except Exception as e:
        logger.error(f"è·å–è§†é¢‘åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/videos/<path:filename>')
def serve_video(filename):
    """æä¾›è§†é¢‘æ–‡ä»¶"""
    return send_from_directory(UPLOAD_FOLDER, filename)

@app.route('/api/upload', methods=['POST'])
def upload_video():
    """ä¸Šä¼ è§†é¢‘"""
    if 'video' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
    
    file = request.files['video']
    if file.filename == '':
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    if file and allowed_file(file.filename, ALLOWED_VIDEO_EXTENSIONS):
        filename = secure_filename(file.filename)
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        file.save(filepath)
        
        return jsonify({
            'name': filename,
            'url': f'/videos/{filename}'
        })
    
    return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400

@app.route('/api/speech-to-text', methods=['POST'])
def speech_to_text():
    """ä½¿ç”¨ DashScope SDK è¿›è¡Œè¯­éŸ³è¯†åˆ«"""
    logger.info("æ”¶åˆ°è¯­éŸ³è¯†åˆ«è¯·æ±‚")
    
    if not DASHSCOPE_API_KEY:
        logger.error("æœªé…ç½® DASHSCOPE_API_KEY")
        return jsonify({
            'error': 'æœªé…ç½® DASHSCOPE_API_KEY',
            'message': 'è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DASHSCOPE_API_KEY',
            'transcript': ''
        }), 500
    
    if 'audio' not in request.files:
        logger.error("æ²¡æœ‰æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶")
        return jsonify({
            'error': 'æ²¡æœ‰éŸ³é¢‘æ–‡ä»¶',
            'transcript': ''
        }), 400
    
    audio_file = request.files['audio']
    
    # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    temp_file = tempfile.NamedTemporaryFile(
        delete=False, 
        suffix='.pcm', 
        dir=TEMP_AUDIO_FOLDER
    )
    temp_filepath = temp_file.name
    temp_file.close()  # å…³é—­æ–‡ä»¶ä»¥ä¾¿å†™å…¥
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘
        audio_file.save(temp_filepath)
        file_size = os.path.getsize(temp_filepath)
        logger.info(f"éŸ³é¢‘æ–‡ä»¶è·¯å¾„: {temp_filepath}")
        logger.info(f"éŸ³é¢‘æ–‡ä»¶å¤§å°: {file_size} bytes")
        
        # ä½¿ç”¨ DashScope SDK è¿›è¡Œè¯†åˆ«
        logger.info("æ­£åœ¨è°ƒç”¨ DashScope ASR API...")
        
        # åˆ›å»º Recognition å¯¹è±¡
        recognition = Recognition(
            model='paraformer-realtime-v2',
            format='pcm',
            sample_rate=16000,
            callback=None  # åŒæ­¥è°ƒç”¨
        )
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶å†…å®¹ï¼ˆä½œä¸ºäºŒè¿›åˆ¶æ•°æ®ï¼‰
        with open(temp_filepath, 'rb') as f:
            audio_data = f.read()
            logger.info(f"éŸ³é¢‘æ•°æ®é•¿åº¦: {len(audio_data)} bytes")

        result = recognition.call(temp_filepath)  # âœ… ä¼ å…¥ str
        
        logger.info(f"DashScope API å“åº”çŠ¶æ€: {result.get('status_code', 'unknown')}")
        logger.info(f"DashScope API å®Œæ•´å“åº”: {json.dumps(result, ensure_ascii=False, indent=2)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if isinstance(result, dict) and result.get('status_code') != 200:
            error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
            logger.error(f"API è¿”å›é”™è¯¯: {error_msg}")
            return jsonify({
                'error': 'API è°ƒç”¨å¤±è´¥',
                'message': error_msg,
                'transcript': '',
                'raw': result
            }), 500
        
        # æå–è¯†åˆ«ç»“æœ
        transcript = extract_transcript(result)
        
        if not transcript or transcript.strip() == '':
            logger.info("è¯†åˆ«ç»“æœä¸ºç©º")
            return jsonify({
                'transcript': '',
                'message': 'æœªèƒ½è¯†åˆ«å‡ºè¯­éŸ³ï¼Œè¯·ç¡®ä¿æ¸…æ™°è¯´è¯å¹¶é è¿‘éº¦å…‹é£',
                'raw': result
            })
        
        logger.info(f"âœ… è¯†åˆ«æˆåŠŸ: {transcript}")
        return jsonify({
            'transcript': transcript.strip(),
            'raw': result
        })
        
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
        logger.exception(e)
        
        return jsonify({
            'error': 'è¯­éŸ³è¯†åˆ«å¤±è´¥',
            'message': str(e),
            'transcript': ''
        }), 500
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_filepath):
                os.unlink(temp_filepath)
                logger.info("å·²åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶")
        except Exception as e:
            logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

def extract_transcript(result):
    """ä» DashScope å“åº”ä¸­æå–è¯†åˆ«æ–‡æœ¬"""
    if not result:
        return ''
    
    try:
        # ç¡®ä¿ result æ˜¯å­—å…¸
        if not isinstance(result, dict):
            logger.warning(f"å“åº”ä¸æ˜¯å­—å…¸ç±»å‹: {type(result)}")
            return ''
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯çŠ¶æ€ç 
        status_code = result.get('status_code')
        if status_code and status_code != 200:
            logger.error(f"API è¿”å›é”™è¯¯çŠ¶æ€ç : {status_code}")
            return ''
        
        # å°è¯•ä» output ä¸­æå–
        output = result.get('output')
        if not output:
            logger.warning("å“åº”ä¸­æ²¡æœ‰ output å­—æ®µ")
            return ''
        
        # æ–¹å¼1: output.text (æœ€å¸¸è§)
        if isinstance(output, dict) and 'text' in output:
            text = output['text']
            if isinstance(text, str) and text:
                logger.info(f"ä» output.text æå–: {text}")
                return text
        
        # æ–¹å¼2: output.sentence.text
        if isinstance(output, dict) and 'sentence' in output:
            sentence = output['sentence']
            if isinstance(sentence, dict) and 'text' in sentence:
                text = sentence['text']
                if isinstance(text, str) and text:
                    logger.info(f"ä» output.sentence.text æå–: {text}")
                    return text
        
        # æ–¹å¼3: output ç›´æ¥æ˜¯å­—ç¬¦ä¸²
        if isinstance(output, str) and output:
            logger.info(f"output ç›´æ¥æ˜¯å­—ç¬¦ä¸²: {output}")
            return output
        
        # æ–¹å¼4: output.results æ•°ç»„
        if isinstance(output, dict) and 'results' in output:
            results = output['results']
            if isinstance(results, list) and len(results) > 0:
                texts = []
                for r in results:
                    if isinstance(r, dict):
                        # å°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µå
                        text = (r.get('text') or 
                               r.get('transcription_text') or 
                               r.get('transcript') or
                               '')
                        
                        # å¦‚æœæœ‰åµŒå¥—çš„ sentence
                        if not text and 'sentence' in r:
                            sentence = r['sentence']
                            if isinstance(sentence, dict):
                                text = sentence.get('text', '')
                        
                        if text and isinstance(text, str):
                            texts.append(text)
                
                if texts:
                    combined = ''.join(texts)
                    logger.info(f"ä» output.results æå–: {combined}")
                    return combined
        
        logger.warning(f"æ— æ³•ä»å“åº”ä¸­æå–æ–‡æœ¬ï¼Œå“åº”ç»“æ„: {json.dumps(result, ensure_ascii=False)[:500]}")
        return ''
        
    except Exception as e:
        logger.error(f"æå–æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        logger.exception(e)
        return ''

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'ok',
        'dashscopeConfigured': bool(DASHSCOPE_API_KEY),
        'sdkVersion': dashscope.__version__ if hasattr(dashscope, '__version__') else 'unknown',
        'timestamp': os.popen('date').read().strip()
    })

@app.route('/api/llm-match', methods=['POST'])
def llm_match():
    """Use LLM to match user speech to best video response"""
    try:
        data = request.json
        user_speech = data.get('user_speech', '')
        videos = data.get('videos', [])

        if not user_speech or not videos:
            return jsonify({'error': 'Missing user_speech or videos'}), 400

        logger.info(f"LLM matching: user_speech='{user_speech}', {len(videos)} videos")

        # Build prompt for Qwen
        prompt = build_llm_matching_prompt(user_speech, videos)

        # Call Qwen model
        response = Generation.call(
            model='qwen-turbo',
            prompt=prompt
        )

        if response.status_code != 200:
            logger.error(f"Qwen API error: {response.message}")
            return jsonify({'error': 'LLM API failed', 'message': response.message}), 500

        # Parse LLM response
        llm_output = response.output.text.strip()
        logger.info(f"LLM output: {llm_output}")

        # Extract matched index from LLM response
        matched_index, confidence, reason = parse_llm_response(llm_output, videos)

        logger.info(f"LLM match result: index={matched_index}, confidence={confidence}, reason={reason}")

        return jsonify({
            'matched_index': matched_index,
            'confidence': confidence,
            'reason': reason,
            'llm_output': llm_output
        })

    except Exception as e:
        logger.error(f"LLM matching error: {e}")
        logger.exception(e)
        return jsonify({'error': str(e)}), 500


def build_llm_matching_prompt(user_speech, videos):
    """Build prompt for LLM to match user speech to video responses"""

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¯¹è¯åŒ¹é…åŠ©æ‰‹ã€‚ç”¨æˆ·è¯´äº†ä¸€å¥è¯ï¼Œä½ éœ€è¦ä»å¤šä¸ªè§†é¢‘å›å¤ä¸­é€‰æ‹©æœ€åˆé€‚çš„å›åº”ã€‚

ç”¨æˆ·è¯´: "{user_speech}"

å¯é€‰çš„è§†é¢‘å›å¤:
"""

    for i, video in enumerate(videos):
        prompt += f"\n{i}. {video['name']}\n   å†…å®¹: {video['transcript'][:200]}\n"

    prompt += """
è¯·åˆ†æç”¨æˆ·çš„è¯ï¼Œé€‰æ‹©æœ€åˆé€‚çš„è§†é¢‘ä½œä¸ºå›åº”ã€‚è€ƒè™‘:
1. å¯¹è¯çš„è¿è´¯æ€§å’Œè‡ªç„¶æ€§
2. æƒ…æ„Ÿå’Œè¯­æ°”çš„åŒ¹é…
3. ä¸Šä¸‹æ–‡çš„åˆç†æ€§

è¯·åªè¿”å›ä¸€ä¸ªJSONæ ¼å¼çš„å›ç­”:
{
  "index": é€‰ä¸­çš„è§†é¢‘ç´¢å¼•(æ•°å­—),
  "confidence": ç½®ä¿¡åº¦(0-1ä¹‹é—´çš„å°æ•°),
  "reason": "é€‰æ‹©ç†ç”±(ç®€çŸ­è¯´æ˜)"
}

å¦‚æœæ²¡æœ‰åˆé€‚çš„åŒ¹é…ï¼Œè¿”å›:
{
  "index": -1,
  "confidence": 0,
  "reason": "æ²¡æœ‰åˆé€‚çš„å›åº”"
}
"""

    return prompt


def parse_llm_response(llm_output, videos):
    """Parse LLM response to extract matched index"""
    try:
        # Try to parse as JSON
        import re

        # Find JSON in the response
        json_match = re.search(r'\{[^}]+\}', llm_output)
        if json_match:
            result = json.loads(json_match.group())
            index = result.get('index', -1)
            confidence = result.get('confidence', 0.95)
            reason = result.get('reason', 'LLM matched')

            # Validate index
            if index >= 0 and index < len(videos):
                return index, confidence, reason

        # Fallback: try to find number in response
        numbers = re.findall(r'\b(\d+)\b', llm_output)
        if numbers:
            index = int(numbers[0])
            if index >= 0 and index < len(videos):
                return index, 0.85, 'Extracted from LLM response'

        return -1, 0, 'No match found'

    except Exception as e:
        logger.error(f"Error parsing LLM response: {e}")
        return -1, 0, f'Parse error: {str(e)}'

# ============================================================================
# WebSocket Streaming Implementation
# ============================================================================

class StreamingRecognitionCallback(RecognitionCallback):
    """Callback handler for streaming recognition results"""

    def __init__(self, session_id):
        self.session_id = session_id
        self.partial_results = []
        self.final_result = None

    def on_open(self):
        """Called when recognition stream opens"""
        logger.info(f"[{self.session_id}] Recognition stream opened")
        socketio.emit('recognition_opened', {
            'session_id': self.session_id,
            'status': 'opened'
        })

    def on_event(self, result):
        """Called for each recognition result (partial or final)"""
        try:
            logger.info(f"[{self.session_id}] Recognition event received")

            # Extract transcript from result
            transcript = self._extract_transcript(result)

            if transcript:
                logger.info(f"[{self.session_id}] ğŸ“ Transcript: {transcript}")

                # Send partial result to client
                socketio.emit('recognition_result', {
                    'session_id': self.session_id,
                    'transcript': transcript,
                    'is_final': False
                })

                self.partial_results.append(transcript)
                self.final_result = transcript
        except Exception as e:
            logger.error(f"[{self.session_id}] Error in on_event: {e}")
            logger.exception(e)

    def on_complete(self):
        """Called when recognition completes"""
        logger.info(f"[{self.session_id}] Recognition completed")

        # Send final result
        final_transcript = self.final_result or ''
        logger.info(f"[{self.session_id}] âœ… Final transcript: {final_transcript}")

        socketio.emit('recognition_complete', {
            'session_id': self.session_id,
            'transcript': final_transcript,
            'is_final': True
        })

    def on_error(self, result):
        """Called on recognition error"""
        logger.error(f"[{self.session_id}] Recognition error: {result}")
        socketio.emit('recognition_error', {
            'session_id': self.session_id,
            'error': str(result)
        })

    def on_close(self):
        """Called when recognition stream closes"""
        logger.info(f"[{self.session_id}] Recognition stream closed")
        socketio.emit('recognition_closed', {
            'session_id': self.session_id,
            'status': 'closed'
        })

    def _extract_transcript(self, result):
        """Extract transcript from recognition result"""
        try:
            if not result:
                return ''

            # For streaming callbacks, result is RecognitionResult object
            # Try to access output directly
            output = None

            if hasattr(result, 'output'):
                output = result.output
            elif hasattr(result, 'response') and hasattr(result.response, 'output'):
                output = result.response.output
            elif isinstance(result, dict):
                output = result.get('output')

            if not output:
                logger.warning(f"[{self.session_id}] No output in result: {type(result)}, {dir(result)}")
                return ''

            # Try different output formats
            if isinstance(output, dict):
                # Method 1: output.sentence array (for file-based)
                if 'sentence' in output:
                    sentences = output['sentence']
                    if isinstance(sentences, list):
                        texts = []
                        for sentence in sentences:
                            if isinstance(sentence, dict) and 'text' in sentence:
                                texts.append(sentence['text'])
                        if texts:
                            return ''.join(texts)
                    elif isinstance(sentences, dict) and 'text' in sentences:
                        return sentences['text']

                # Method 2: output.text (most common for streaming)
                if 'text' in output and output['text']:
                    return output['text']

            # Method 3: output is string
            if isinstance(output, str):
                return output

            return ''
        except Exception as e:
            logger.error(f"[{self.session_id}] Error extracting transcript: {e}")
            logger.exception(e)
            return ''

# Store active recognition sessions
active_sessions = {}
sessions_lock = Lock()

@socketio.on('connect')
def handle_connect():
    """Handle WebSocket connection"""
    logger.info(f"Client connected: {request.sid}")
    emit('connected', {'session_id': request.sid})

@socketio.on('disconnect')
def handle_disconnect():
    """Handle WebSocket disconnection"""
    logger.info(f"Client disconnected: {request.sid}")

    # Clean up any active recognition session
    with sessions_lock:
        if request.sid in active_sessions:
            try:
                recognition = active_sessions[request.sid]
                recognition.stop()
                del active_sessions[request.sid]
                logger.info(f"Cleaned up recognition session: {request.sid}")
            except Exception as e:
                logger.error(f"Error cleaning up session: {e}")

@socketio.on('start_recognition')
def handle_start_recognition(data=None):
    """Start a new recognition session"""
    session_id = request.sid
    logger.info(f"[{session_id}] Starting recognition session")

    if not DASHSCOPE_API_KEY:
        emit('recognition_error', {
            'error': 'DASHSCOPE_API_KEY not configured'
        })
        return

    try:
        with sessions_lock:
            # Stop any existing session
            if session_id in active_sessions:
                try:
                    active_sessions[session_id].stop()
                except:
                    pass

            # Create new recognition instance with callback
            callback = StreamingRecognitionCallback(session_id)
            recognition = Recognition(
                model='paraformer-realtime-v2',
                format='pcm',
                sample_rate=16000,
                callback=callback
            )

            # Start recognition
            recognition.start()

            # Initialize counters
            recognition._audio_frame_count = 0
            recognition._total_bytes_sent = 0

            # Store session
            active_sessions[session_id] = recognition

            logger.info(f"[{session_id}] Recognition started successfully")
            emit('recognition_started', {
                'session_id': session_id,
                'status': 'started'
            })

    except Exception as e:
        logger.error(f"[{session_id}] Failed to start recognition: {e}")
        logger.exception(e)
        emit('recognition_error', {
            'error': str(e)
        })

@socketio.on('audio_data')
def handle_audio_data(data):
    """Receive and process audio data chunks"""
    session_id = request.sid

    try:
        with sessions_lock:
            if session_id not in active_sessions:
                logger.warning(f"[{session_id}] No active recognition session")
                emit('recognition_error', {
                    'error': 'No active recognition session'
                })
                return

            recognition = active_sessions[session_id]

        # data should be Int16Array sent as array of numbers
        if isinstance(data, dict) and 'audio' in data:
            # Convert Int16 array to bytes
            # Int16 values are -32768 to 32767, need to convert to bytes properly
            int16_array = data['audio']

            # Log first time we receive data
            if not hasattr(recognition, '_first_data_logged'):
                logger.info(f"[{session_id}] First audio data received: {len(int16_array)} samples")
                logger.info(f"[{session_id}] Sample values (first 10): {int16_array[:10]}")
                recognition._first_data_logged = True

            # Pack as little-endian signed 16-bit integers
            audio_bytes = struct.pack(f'<{len(int16_array)}h', *int16_array)
        elif isinstance(data, bytes):
            audio_bytes = data
        else:
            logger.error(f"[{session_id}] Invalid audio data format: {type(data)}")
            return

        # Send audio frame to DashScope
        recognition.send_audio_frame(audio_bytes)

        # Update counters
        recognition._audio_frame_count += 1
        recognition._total_bytes_sent += len(audio_bytes)

        # Log every 50 frames
        if recognition._audio_frame_count % 50 == 0:
            logger.info(f"[{session_id}] Sent {recognition._audio_frame_count} frames, {recognition._total_bytes_sent} bytes total")

        logger.debug(f"[{session_id}] Sent {len(audio_bytes)} bytes to recognition")

    except Exception as e:
        logger.error(f"[{session_id}] Error processing audio data: {e}")
        logger.exception(e)
        emit('recognition_error', {
            'error': str(e)
        })

@socketio.on('stop_recognition')
def handle_stop_recognition(data=None):
    """Stop the recognition session"""
    session_id = request.sid
    logger.info(f"[{session_id}] Stopping recognition session")

    try:
        with sessions_lock:
            if session_id in active_sessions:
                recognition = active_sessions[session_id]

                # Log final stats
                if hasattr(recognition, '_audio_frame_count'):
                    logger.info(f"[{session_id}] Final stats: {recognition._audio_frame_count} frames, {recognition._total_bytes_sent} bytes")

                recognition.stop()
                del active_sessions[session_id]
                logger.info(f"[{session_id}] Recognition stopped successfully")
                emit('recognition_stopped', {
                    'session_id': session_id,
                    'status': 'stopped'
                })
            else:
                logger.warning(f"[{session_id}] No active session to stop")

    except Exception as e:
        logger.error(f"[{session_id}] Error stopping recognition: {e}")
        logger.exception(e)
        emit('recognition_error', {
            'error': str(e)
        })

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ¬ è¯­éŸ³äº¤äº’è§†é¢‘æ’­æ”¾å™¨ - Python ç‰ˆæœ¬")
    print("=" * 60)
    print()
    print("ğŸ“ è§†é¢‘ç›®å½•:", UPLOAD_FOLDER)
    print("ğŸ”‘ DashScope API Key:", 'âœ… å·²é…ç½®' if DASHSCOPE_API_KEY else 'âŒ æœªé…ç½®')
    
    if DASHSCOPE_API_KEY:
        print(f"   API Key å‰ç¼€: {DASHSCOPE_API_KEY[:10]}...")
    
    try:
        sdk_version = dashscope.__version__ if hasattr(dashscope, '__version__') else 'unknown'
        print(f"ğŸ“¦ DashScope SDK ç‰ˆæœ¬: {sdk_version}")
    except:
        print("ğŸ“¦ DashScope SDK ç‰ˆæœ¬: unknown")
    
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:5001")
    print()
    
    if not DASHSCOPE_API_KEY:
        print("âš ï¸  è­¦å‘Š: æœªé…ç½® DASHSCOPE_API_KEY")
        print("   è¯·åˆ›å»º .env æ–‡ä»¶å¹¶æ·»åŠ :")
        print("   DASHSCOPE_API_KEY=sk-your-key-here")
        print()
        print("   è·å– API Key: https://dashscope.console.aliyun.com/apiKey")
        print()
    
    print("ğŸ’¡ æç¤º:")
    print("   - æµ‹è¯•é…ç½®: python test_dashscope.py")
    print("   - æµ‹è¯•éŸ³é¢‘: python test_audio.py <éŸ³é¢‘æ–‡ä»¶>")
    print("   - æŸ¥çœ‹æ—¥å¿—: ç›´æ¥æŸ¥çœ‹æ§åˆ¶å°è¾“å‡º")
    print()
    print("=" * 60)
    print()

    socketio.run(app, host='0.0.0.0', port=5001, debug=True, allow_unsafe_werkzeug=True)